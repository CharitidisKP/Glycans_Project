{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f099f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significantly Increased Variables:\n",
      "   Variable  T-statistic   P-value  Direction  Significant\n",
      "2       GP3     3.603668  0.002031  increased         True\n",
      "3       GP4     4.278185  0.000452  increased         True\n",
      "4       GP5     3.689974  0.001676  increased         True\n",
      "6       GP7     5.245505  0.000055  increased         True\n",
      "7       GP8     2.765208  0.012752  increased         True\n",
      "8       GP9     6.535331  0.000004  increased         True\n",
      "10     GP11     3.517142  0.002461  increased         True\n",
      "12     GP13     2.591255  0.018435  increased         True\n",
      "31       S1     2.397891  0.027546  increased         True\n",
      "32       S2     4.152243  0.000598  increased         True\n",
      "\n",
      "Significantly Decreased Variables:\n",
      "   Variable  T-statistic   P-value  Direction  Significant\n",
      "15     GP16    -4.287478  0.000443  decreased         True\n",
      "18     GP19    -2.922596  0.009090  decreased         True\n",
      "22     GP23    -4.095926  0.000678  decreased         True\n",
      "24     GP25    -2.347401  0.030542  decreased         True\n",
      "26     GP27    -2.211336  0.040189  decreased         True\n",
      "28       G1    -2.285451  0.034632  decreased         True\n",
      "30       S0    -2.708579  0.014387  decreased         True\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Step 2: Load the data\n",
    "file_path = 'LN_total_IgG_glycomedata.xlsx'  # Replace with the actual file path\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Step 3: Reshape the data\n",
    "# Assuming the columns are structured as you mentioned\n",
    "patient_col = \"Patient number\"\n",
    "time_col = \"Time point\"\n",
    "variables = data.columns[4:]  # Assuming variables start from column E\n",
    "\n",
    "# Step 4: Perform paired t-tests\n",
    "results = []\n",
    "\n",
    "for variable in variables:\n",
    "    # Extract T0 and T12 for the current variable\n",
    "    t0_values = data[data[time_col] == \"T0\"].sort_values(by=patient_col)[variable].values\n",
    "    t12_values = data[data[time_col] == \"T12\"].sort_values(by=patient_col)[variable].values\n",
    "\n",
    "    # Perform paired t-test\n",
    "    t_stat, p_value = ttest_rel(t0_values, t12_values)\n",
    "\n",
    "    # Determine significance and direction\n",
    "    direction = \"increased\" if t_stat > 0 else \"decreased\"\n",
    "    results.append({\"Variable\": variable, \"T-statistic\": t_stat, \"P-value\": p_value, \"Direction\": direction})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 5: Identify significant changes\n",
    "significance_level = 0.05\n",
    "results_df[\"Significant\"] = results_df[\"P-value\"] < significance_level\n",
    "\n",
    "# Separate increased and decreased\n",
    "increased = results_df[(results_df[\"Significant\"]) & (results_df[\"Direction\"] == \"increased\")]\n",
    "decreased = results_df[(results_df[\"Significant\"]) & (results_df[\"Direction\"] == \"decreased\")]\n",
    "\n",
    "# Output results\n",
    "print(\"Significantly Increased Variables:\")\n",
    "print(increased)\n",
    "print(\"\\nSignificantly Decreased Variables:\")\n",
    "print(decreased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638de7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad4748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15a17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98092240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440ae96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9ed52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83dbc022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significantly increased variables saved to significant_increased_variables.xlsx\n",
      "Significantly decreased variables saved to significant_decreased_variables.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Identify significant changes\n",
    "significance_level = 0.05\n",
    "results_df[\"Significant\"] = results_df[\"P-value\"] < significance_level\n",
    "\n",
    "# Separate increased and decreased\n",
    "increased = results_df[(results_df[\"Significant\"]) & (results_df[\"Direction\"] == \"increased\")]\n",
    "decreased = results_df[(results_df[\"Significant\"]) & (results_df[\"Direction\"] == \"decreased\")]\n",
    "\n",
    "# Export results to Excel\n",
    "increased_file = \"significant_increased_variables.xlsx\"\n",
    "decreased_file = \"significant_decreased_variables.xlsx\"\n",
    "\n",
    "increased.to_excel(increased_file, index=False)\n",
    "decreased.to_excel(decreased_file, index=False)\n",
    "\n",
    "print(f\"Significantly increased variables saved to {increased_file}\")\n",
    "print(f\"Significantly decreased variables saved to {decreased_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3d389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0a172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092abdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7547dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd72414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05bbc4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to updated_dataset_with_significant_hits.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
      "/var/folders/jy/7nq00pfn6t1byp_8dl77dwcm0000gn/T/ipykernel_26655/3468807391.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Identify significant changes\n",
    "significance_level = 0.05\n",
    "results_df[\"Significant\"] = results_df[\"P-value\"] < significance_level\n",
    "\n",
    "# Prepare lists of significant variables\n",
    "increased_vars = results_df[(results_df[\"Significant\"]) & (results_df[\"Direction\"] == \"increased\")][\"Variable\"].tolist()\n",
    "decreased_vars = results_df[(results_df[\"Significant\"]) & (results_df[\"Direction\"] == \"decreased\")][\"Variable\"].tolist()\n",
    "\n",
    "# Initialize new columns for each variable's significance\n",
    "for variable in data.columns[4:]:  # Assuming variables start at column E\n",
    "    data[f\"{variable}_Significantly Increased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(increased_vars).astype(int)\n",
    "    data[f\"{variable}_Significantly Decreased\"] = data[variable].where(data[\"Time point\"] == \"T12\").isin(decreased_vars).astype(int)\n",
    "\n",
    "# Save the updated dataset with significance annotations\n",
    "output_file = \"updated_dataset_with_significant_hits.xlsx\"\n",
    "data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81ba464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset with significantly increased variables saved to variables_with_significant_increase.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Identify significant changes\n",
    "significance_level = 0.05\n",
    "results_df[\"Significant\"] = results_df[\"P-value\"] < significance_level\n",
    "\n",
    "# Get the list of variables that significantly increased\n",
    "increased_vars = results_df[(results_df[\"Significant\"]) & (results_df[\"Direction\"] == \"increased\")][\"Variable\"].tolist()\n",
    "\n",
    "# Filter the dataset to include only these variables and the key columns\n",
    "filtered_data = data[[\"Patient number\", \"Time point\"] + increased_vars]\n",
    "\n",
    "# Save the filtered dataset\n",
    "output_file = \"variables_with_significant_increase.xlsx\"\n",
    "filtered_data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered dataset with significantly increased variables saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "414699bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset with significantly increased variables saved to variables_with_significant_increase.xlsx\n",
      "Filtered dataset with significantly decreased variables saved to variables_with_significant_decrease.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Identify significant changes\n",
    "significance_level = 0.05\n",
    "results_df[\"Significant\"] = results_df[\"P-value\"] < significance_level\n",
    "\n",
    "# Get the list of variables that significantly increased and decreased\n",
    "increased_vars = results_df[(results_df[\"Significant\"]) & (results_df[\"Direction\"] == \"increased\")][\"Variable\"].tolist()\n",
    "decreased_vars = results_df[(results_df[\"Significant\"]) & (results_df[\"Direction\"] == \"decreased\")][\"Variable\"].tolist()\n",
    "\n",
    "# Filter the dataset for significantly increased variables\n",
    "increased_data = data[[\"Patient number\", \"Time point\"] + increased_vars]\n",
    "increased_output_file = \"variables_with_significant_increase.xlsx\"\n",
    "increased_data.to_excel(increased_output_file, index=False)\n",
    "print(f\"Filtered dataset with significantly increased variables saved to {increased_output_file}\")\n",
    "\n",
    "# Filter the dataset for significantly decreased variables\n",
    "decreased_data = data[[\"Patient number\", \"Time point\"] + decreased_vars]\n",
    "decreased_output_file = \"variables_with_significant_decrease.xlsx\"\n",
    "decreased_data.to_excel(decreased_output_file, index=False)\n",
    "print(f\"Filtered dataset with significantly decreased variables saved to {decreased_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8d416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793b485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b09271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412c735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990514d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e451e830",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'T12'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'T12'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelta values saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate delta for significantly increased variables\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mcalculate_delta_for_filtered_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvariables_with_significant_increase.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta_significant_increase.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate delta for significantly decreased variables\u001b[39;00m\n\u001b[1;32m     25\u001b[0m calculate_delta_for_filtered_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables_with_significant_decrease.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_significant_decrease.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mcalculate_delta_for_filtered_data\u001b[0;34m(input_file, output_file)\u001b[0m\n\u001b[1;32m     12\u001b[0m pivoted_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatient number\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime point\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39mvariables)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate delta (T12 - T0)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m delta_data \u001b[38;5;241m=\u001b[39m \u001b[43mpivoted_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mT12\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m pivoted_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Save delta data to an Excel file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m delta_data\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mto_excel(output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3806\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[1;32m   3805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 3806\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_multilevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3857\u001b[0m, in \u001b[0;36mDataFrame._getitem_multilevel\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3855\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_multilevel\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   3856\u001b[0m     \u001b[38;5;66;03m# self.columns is a MultiIndex\u001b[39;00m\n\u001b[0;32m-> 3857\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, (\u001b[38;5;28mslice\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[1;32m   3859\u001b[0m         new_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[loc]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/multi.py:2916\u001b[0m, in \u001b[0;36mMultiIndex.get_loc\u001b[0;34m(self, key, method)\u001b[0m\n\u001b[1;32m   2913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n\u001b[1;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m-> 2916\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_level_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_to_slice(loc)\n\u001b[1;32m   2919\u001b[0m keylen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/multi.py:3263\u001b[0m, in \u001b[0;36mMultiIndex._get_level_indexer\u001b[0;34m(self, key, level, indexer)\u001b[0m\n\u001b[1;32m   3259\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(i, j, step)\n\u001b[1;32m   3261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3263\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_loc_single_level_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexsort_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3266\u001b[0m         \u001b[38;5;66;03m# Desired level is not sorted\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   3268\u001b[0m             \u001b[38;5;66;03m# test_get_loc_partial_timestamp_multiindex\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/multi.py:2849\u001b[0m, in \u001b[0;36mMultiIndex._get_loc_single_level_index\u001b[0;34m(self, level_index, key)\u001b[0m\n\u001b[1;32m   2847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlevel_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'T12'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to calculate delta (T12 - T0)\n",
    "def calculate_delta_for_filtered_data(input_file, output_file):\n",
    "    # Load the filtered data\n",
    "    data = pd.read_excel(input_file)\n",
    "\n",
    "    # Ensure that the dataset contains only \"Patient number\", \"Time point\", and the measured variables\n",
    "    variables = data.columns[2:]  # Assuming that the first two columns are \"Patient number\" and \"Time point\"\n",
    "\n",
    "    # Pivot the data to have T0 and T12 as columns for each patient\n",
    "    pivoted_data = data.pivot(index=\"Patient number\", columns=\"Time point\", values=variables)\n",
    "\n",
    "    # Calculate delta (T12 - T0)\n",
    "    delta_data = pivoted_data[\"T12\"] - pivoted_data[\"T0\"]\n",
    "\n",
    "    # Save delta data to an Excel file\n",
    "    delta_data.reset_index().to_excel(output_file, index=False)\n",
    "    print(f\"Delta values saved to {output_file}\")\n",
    "\n",
    "# Calculate delta for significantly increased variables\n",
    "calculate_delta_for_filtered_data(\"variables_with_significant_increase.xlsx\", \"delta_significant_increase.xlsx\")\n",
    "\n",
    "# Calculate delta for significantly decreased variables\n",
    "calculate_delta_for_filtered_data(\"variables_with_significant_decrease.xlsx\", \"delta_significant_decrease.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a34d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5056f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093388b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
